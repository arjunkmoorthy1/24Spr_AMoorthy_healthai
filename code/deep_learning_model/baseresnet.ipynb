{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pydicom\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(path, img_size=256):\n",
    "    # Load dicom and convert to 256x256 RGB image\n",
    "    dicom = pydicom.dcmread(path)\n",
    "    image = dicom.pixel_array\n",
    "    image = Image.fromarray(image).convert('RGB')\n",
    "    image = image.resize((img_size, img_size))\n",
    "    image = np.array(image) / 255.0\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    print(f\"Loaded image shape: {image.shape}, dtype: {image.dtype}\")\n",
    "    return tf.convert_to_tensor(image, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory_path, label, img_size=256):\n",
    "    images, labels = [], []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for filename in files:\n",
    "            # Check if the file is a DICOM file\n",
    "            if filename.endswith('.dcm'):\n",
    "                # Construct the full path to the DICOM file\n",
    "                file_path = os.path.join(root, filename)\n",
    "                # Load and process the image\n",
    "                image = load_dicom_image(file_path, img_size=img_size)\n",
    "                # Append the processed image and its label to the lists\n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(cancer_dir, non_cancer_dir, img_size=256):\n",
    "    # Load and process images and labels\n",
    "    cancer_images, cancer_labels = process_directory(cancer_dir, 1, img_size=img_size)\n",
    "    non_cancer_images, non_cancer_labels = process_directory(non_cancer_dir, 0, img_size=img_size)\n",
    "    \n",
    "    # Combine images and labels\n",
    "    all_images = np.concatenate([np.array(cancer_images), np.array(non_cancer_images)], axis=0)\n",
    "    all_labels = np.array(cancer_labels + non_cancer_labels)\n",
    "\n",
    "    # After combining images and labels\n",
    "    print(f\"all_images shape: {all_images.shape}, dtype: {all_images.dtype}\")\n",
    "    print(f\"all_labels shape: {all_labels.shape}, dtype: {all_labels.dtype}\")\n",
    "\n",
    "    \n",
    "    return all_images, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_custom_model(input_shape=(256, 256, 3), dropout_rate=0.5, l2_reg=0.001):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Example of a simple CNN architecture\n",
    "    x = Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "all_images, all_labels = create_datasets(\n",
    "    cancer_dir='/Users/arjunmoorthy/Desktop/Research_Capstone/ImageData/CapstoneData/cancer',\n",
    "    non_cancer_dir='/Users/arjunmoorthy/Desktop/Research_Capstone/ImageData/CapstoneData/non_cancer',\n",
    "    img_size=256)\n",
    "\n",
    "# Split data into training+validation and test sets\n",
    "(train_val_images, test_images, train_val_labels, test_labels) = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare for 5-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "# Assuming kf, train_val_images, and train_val_labels are already defined\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_images)):\n",
    "    print(f\"Training dataset shape: {train_val_images[train_idx].shape}\")\n",
    "    print(f\"Training labels shape: {train_val_labels[train_idx].shape}\")\n",
    "    print(f\"Training on fold {fold+1}/5...\")\n",
    "    \n",
    "    # Generate datasets for the current fold\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_val_images[train_idx], train_val_labels[train_idx]))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_idx)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((train_val_images[val_idx], train_val_labels[val_idx]))\n",
    "    val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Build and compile the custom model instead of the ResNet-based model\n",
    "    model = build_custom_model(input_shape=(256, 256, 3), dropout_rate=0.5, l2_reg=0.001)\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(train_dataset, epochs=50, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cross-validation, you might want to retrain your model on all training data and evaluate it on the test set\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)\n",
    "# Make sure to re-build and compile your model\n",
    "\n",
    "# Assuming 'model' is your final trained model\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
