{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(path, img_size=256):\n",
    "    # Load dicom and convert to 256x256 RGB image\n",
    "    dicom = pydicom.dcmread(path)\n",
    "    image = dicom.pixel_array\n",
    "    image = Image.fromarray(image).convert('RGB')\n",
    "    image = image.resize((img_size, img_size))\n",
    "    return np.array(image) / 255.0\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    return tf.convert_to_tensor(image, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_images(patient_folder, img_size=256):\n",
    "    \"\"\"\n",
    "    Process all DICOM images of a patient and return a single representative array.\n",
    "    This is a placeholder function prior to deciding how to handle multiple images with a particular patient\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(patient_folder):\n",
    "        if filename.endswith('.dcm'):\n",
    "            path = os.path.join(patient_folder, filename)\n",
    "            image = load_dicom_image(path, img_size=img_size)\n",
    "            images.append(image)\n",
    "    \n",
    "    # Placeholder: simply return the first image for now\n",
    "    if images:\n",
    "        return images[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_datasets(root_dir, img_size=256, batch_size=32, val_split=0.2):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    patient_folders = []\n",
    "    for condition in os.listdir(root_dir):\n",
    "        condition_path = os.path.join(root_dir, condition)\n",
    "        if not os.path.isdir(condition_path):\n",
    "            continue\n",
    "        \n",
    "        label = 1 if condition.lower() == 'cancer' else 0\n",
    "        for patient_folder_name in os.listdir(condition_path):\n",
    "            patient_folder_path = os.path.join(condition_path, patient_folder_name)\n",
    "            if os.path.isdir(patient_folder_path):\n",
    "                patient_folders.append((patient_folder_path, label))\n",
    "    \n",
    "    # Split patient folders into training and validation sets\n",
    "    train_folders, val_folders = train_test_split(patient_folders, test_size=val_split, random_state=42)\n",
    "\n",
    "    # Process training data\n",
    "    for folder_path, label in train_folders:\n",
    "        image = process_patient_images(folder_path, img_size=img_size)\n",
    "        if image is not None:\n",
    "            all_images.append(image)\n",
    "            all_labels.append(label)\n",
    "    \n",
    "    # Convert training lists to tensors and create a dataset\n",
    "    train_images = tf.stack(all_images)\n",
    "    train_labels = tf.convert_to_tensor(all_labels, dtype=tf.float32)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Reset for validation data processing\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Process validation data\n",
    "    for folder_path, label in val_folders:\n",
    "        image = process_patient_images(folder_path, img_size=img_size)\n",
    "        if image is not None:\n",
    "            all_images.append(image)\n",
    "            all_labels.append(label)\n",
    "    \n",
    "    # Convert validation lists to tensors and create a dataset\n",
    "    val_images = tf.stack(all_images)\n",
    "    val_labels = tf.convert_to_tensor(all_labels, dtype=tf.float32)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def create_dataset(root_dir, img_size=256, batch_size=32):\n",
    "    # Placeholder lists for images and labels\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Navigate through the root directory and process each patient's images\n",
    "    for condition in os.listdir(root_dir):\n",
    "        condition_path = os.path.join(root_dir, condition)\n",
    "        if not os.path.isdir(condition_path):\n",
    "            continue\n",
    "        \n",
    "        label = 1 if condition.lower() == 'cancer' else 0\n",
    "        \n",
    "        for patient_folder_name in os.listdir(condition_path):\n",
    "            patient_folder_path = os.path.join(condition_path, patient_folder_name)\n",
    "            if not os.path.isdir(patient_folder_path):\n",
    "                continue\n",
    "            \n",
    "            representative_image = process_patient_images(patient_folder_path, img_size=img_size)\n",
    "            if representative_image is not None:\n",
    "                all_images.append(representative_image)\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    dataset_images = tf.stack(all_images)\n",
    "    dataset_labels = tf.convert_to_tensor(all_labels, dtype=tf.float32)\n",
    "    \n",
    "    # Create a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataset_images, dataset_labels))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    return dataset \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_model(input_shape=(256, 256, 3)):\n",
    "    base_model = MobileNetV2(input_shape=input_shape,\n",
    "                             include_top=False,\n",
    "                             weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = create_dataset('/Users/arjunmoorthy/Desktop/Research_Capstone/Image Data/CapstoneData', img_size=256, batch_size=32)\n",
    "train_dataset, val_dataset = create_train_val_datasets(root_dir='/Users/arjunmoorthy/Desktop/Research_Capstone/Image Data/CapstoneData', img_size=256, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/j6d8h1bx4z5cv8c9hc3hv6yw0000gn/T/ipykernel_22170/4149438536.py:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(input_shape=input_shape,\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Or another appropriate loss function\n",
    "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 200ms/step - accuracy: 0.4926 - auc: 0.4862 - loss: 0.8397 - precision: 0.4404 - recall: 0.4683 - val_accuracy: 0.5720 - val_auc: 0.4980 - val_loss: 0.7732 - val_precision: 0.6667 - val_recall: 0.0667\n",
      "Epoch 2/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.5386 - auc: 0.5318 - loss: 0.7317 - precision: 0.5014 - recall: 0.2565 - val_accuracy: 0.4982 - val_auc: 0.4939 - val_loss: 0.7243 - val_precision: 0.4344 - val_recall: 0.4417\n",
      "Epoch 3/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.5470 - auc: 0.5729 - loss: 0.6916 - precision: 0.5099 - recall: 0.4871 - val_accuracy: 0.5424 - val_auc: 0.4843 - val_loss: 0.7350 - val_precision: 0.4524 - val_recall: 0.1583\n",
      "Epoch 4/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 188ms/step - accuracy: 0.5670 - auc: 0.5711 - loss: 0.6826 - precision: 0.5018 - recall: 0.3524 - val_accuracy: 0.5203 - val_auc: 0.4803 - val_loss: 0.7316 - val_precision: 0.4194 - val_recall: 0.2167\n",
      "Epoch 5/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.5799 - auc: 0.6275 - loss: 0.6669 - precision: 0.5683 - recall: 0.2794 - val_accuracy: 0.5018 - val_auc: 0.4644 - val_loss: 0.7379 - val_precision: 0.3846 - val_recall: 0.2083\n",
      "Epoch 6/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.5886 - auc: 0.6204 - loss: 0.6668 - precision: 0.5780 - recall: 0.4059 - val_accuracy: 0.4576 - val_auc: 0.4666 - val_loss: 0.7472 - val_precision: 0.4106 - val_recall: 0.5167\n",
      "Epoch 7/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.6171 - auc: 0.6635 - loss: 0.6505 - precision: 0.5990 - recall: 0.5428 - val_accuracy: 0.4760 - val_auc: 0.4509 - val_loss: 0.7423 - val_precision: 0.3854 - val_recall: 0.3083\n",
      "Epoch 8/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.6235 - auc: 0.6714 - loss: 0.6388 - precision: 0.5949 - recall: 0.3671 - val_accuracy: 0.4686 - val_auc: 0.4457 - val_loss: 0.7466 - val_precision: 0.3846 - val_recall: 0.3333\n",
      "Epoch 9/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.6467 - auc: 0.6956 - loss: 0.6374 - precision: 0.6468 - recall: 0.5393 - val_accuracy: 0.5498 - val_auc: 0.4442 - val_loss: 0.7934 - val_precision: 0.4167 - val_recall: 0.0417\n",
      "Epoch 10/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.6189 - auc: 0.7079 - loss: 0.6287 - precision: 0.6271 - recall: 0.4079 - val_accuracy: 0.5018 - val_auc: 0.4372 - val_loss: 0.7551 - val_precision: 0.4026 - val_recall: 0.2583\n",
      "Epoch 11/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.6650 - auc: 0.7197 - loss: 0.6196 - precision: 0.6673 - recall: 0.5199 - val_accuracy: 0.4280 - val_auc: 0.4310 - val_loss: 0.7667 - val_precision: 0.3871 - val_recall: 0.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.6573 - auc: 0.7167 - loss: 0.6247 - precision: 0.6457 - recall: 0.6261 - val_accuracy: 0.5166 - val_auc: 0.4360 - val_loss: 0.7692 - val_precision: 0.3878 - val_recall: 0.1583\n",
      "Epoch 13/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.6557 - auc: 0.7553 - loss: 0.6032 - precision: 0.6968 - recall: 0.4126 - val_accuracy: 0.4502 - val_auc: 0.4334 - val_loss: 0.7644 - val_precision: 0.3645 - val_recall: 0.3250\n",
      "Epoch 14/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.7094 - auc: 0.7642 - loss: 0.5988 - precision: 0.7178 - recall: 0.6058 - val_accuracy: 0.4945 - val_auc: 0.4333 - val_loss: 0.7711 - val_precision: 0.3867 - val_recall: 0.2417\n",
      "Epoch 15/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.6807 - auc: 0.7591 - loss: 0.5995 - precision: 0.6748 - recall: 0.6115 - val_accuracy: 0.4280 - val_auc: 0.4238 - val_loss: 0.7776 - val_precision: 0.3776 - val_recall: 0.4500\n",
      "Epoch 16/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.7026 - auc: 0.7737 - loss: 0.5910 - precision: 0.6868 - recall: 0.6101 - val_accuracy: 0.4317 - val_auc: 0.4219 - val_loss: 0.7766 - val_precision: 0.3365 - val_recall: 0.2917\n",
      "Epoch 17/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.6761 - auc: 0.7462 - loss: 0.6021 - precision: 0.6578 - recall: 0.6138 - val_accuracy: 0.5129 - val_auc: 0.4249 - val_loss: 0.8056 - val_precision: 0.3421 - val_recall: 0.1083\n",
      "Epoch 18/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.6895 - auc: 0.7577 - loss: 0.6019 - precision: 0.7279 - recall: 0.5408 - val_accuracy: 0.4686 - val_auc: 0.4216 - val_loss: 0.7863 - val_precision: 0.3333 - val_recall: 0.2000\n",
      "Epoch 19/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.7363 - auc: 0.8064 - loss: 0.5699 - precision: 0.7662 - recall: 0.6025 - val_accuracy: 0.4170 - val_auc: 0.4139 - val_loss: 0.7869 - val_precision: 0.3273 - val_recall: 0.3000\n",
      "Epoch 20/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.7017 - auc: 0.7655 - loss: 0.5880 - precision: 0.7008 - recall: 0.6009 - val_accuracy: 0.4539 - val_auc: 0.4183 - val_loss: 0.8706 - val_precision: 0.4358 - val_recall: 0.7917\n",
      "Epoch 21/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.7020 - auc: 0.7903 - loss: 0.5745 - precision: 0.6720 - recall: 0.7385 - val_accuracy: 0.4760 - val_auc: 0.4177 - val_loss: 0.7942 - val_precision: 0.3625 - val_recall: 0.2417\n",
      "Epoch 22/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.7094 - auc: 0.7927 - loss: 0.5721 - precision: 0.7352 - recall: 0.5921 - val_accuracy: 0.4170 - val_auc: 0.4168 - val_loss: 0.7974 - val_precision: 0.3208 - val_recall: 0.2833\n",
      "Epoch 23/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.7501 - auc: 0.8281 - loss: 0.5530 - precision: 0.7670 - recall: 0.6450 - val_accuracy: 0.4207 - val_auc: 0.4127 - val_loss: 0.8000 - val_precision: 0.3445 - val_recall: 0.3417\n",
      "Epoch 24/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.7544 - auc: 0.8364 - loss: 0.5456 - precision: 0.7557 - recall: 0.7002 - val_accuracy: 0.4723 - val_auc: 0.4172 - val_loss: 0.8048 - val_precision: 0.3467 - val_recall: 0.2167\n",
      "Epoch 25/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.7222 - auc: 0.7971 - loss: 0.5668 - precision: 0.7293 - recall: 0.6452 - val_accuracy: 0.4428 - val_auc: 0.4126 - val_loss: 0.8062 - val_precision: 0.3434 - val_recall: 0.2833\n",
      "Epoch 26/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.7689 - auc: 0.8479 - loss: 0.5348 - precision: 0.7877 - recall: 0.6539 - val_accuracy: 0.4244 - val_auc: 0.4115 - val_loss: 0.8196 - val_precision: 0.3816 - val_recall: 0.4833\n",
      "Epoch 27/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.7586 - auc: 0.8368 - loss: 0.5472 - precision: 0.7406 - recall: 0.7016 - val_accuracy: 0.4428 - val_auc: 0.4124 - val_loss: 0.8135 - val_precision: 0.3402 - val_recall: 0.2750\n",
      "Epoch 28/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.7505 - auc: 0.8218 - loss: 0.5469 - precision: 0.7817 - recall: 0.5996 - val_accuracy: 0.4096 - val_auc: 0.4145 - val_loss: 0.8183 - val_precision: 0.3529 - val_recall: 0.4000\n",
      "Epoch 29/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.7494 - auc: 0.8224 - loss: 0.5518 - precision: 0.7498 - recall: 0.7144 - val_accuracy: 0.4170 - val_auc: 0.4139 - val_loss: 0.8190 - val_precision: 0.3443 - val_recall: 0.3500\n",
      "Epoch 30/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.7430 - auc: 0.8238 - loss: 0.5467 - precision: 0.7729 - recall: 0.6119 - val_accuracy: 0.4096 - val_auc: 0.4143 - val_loss: 0.8231 - val_precision: 0.3485 - val_recall: 0.3833\n",
      "Epoch 31/50\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.7567 - auc: 0.8375 - loss: 0.5407 - precision: 0.7615 - recall: 0.7010 - val_accuracy: 0.4391 - val_auc: 0.4124 - val_loss: 0.8240 - val_precision: 0.3333 - val_recall: 0.2667\n",
      "Epoch 32/50\n",
      "\u001b[1m28/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.7458 - auc: 0.8285 - loss: 0.5352 - precision: 0.7800 - recall: 0.5683"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=50,\n",
    "                    validation_data=val_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  \n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation AUC values\n",
    "plt.plot(history.history['auc'], label='Train AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')  \n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')  \n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# dicom_dir_path = '/Users/arjunmoorthy/Desktop/Research_Capstone/Image Data/CapstoneData'  # Directory where DICOM files are stored"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
